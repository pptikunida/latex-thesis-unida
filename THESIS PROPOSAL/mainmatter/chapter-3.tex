\chapter{RESEARCH METHODOLOGY}
	\section{Type and Approach of Research}
	\setlength{\leftskip}{0.7cm}
	\lipsum[3]		
	\section{Object of Research}
	The recent advent and rapid proliferation of Large Language Models (LLMs) mark a pivotal moment in the history of artificial intelligence. These sophisticated neural networks,\footcite{alip2022} predominantly built upon the transformer architecture, are trained on vast corpora of text and code, enabling them to generate human-like text, translate languages, and answer questions with remarkable fluency. This capability signifies a paradigm shift from traditional,\footcite{azizah2024} rule-based AI to more intuitive and generative systems, fundamentally altering the landscape of human-computer interaction. Models such as OpenAI's GPT series and Google's Gemini have demonstrated that by scaling up data and computational power, machines can acquire a nuanced understanding of language and context that was previously thought to be uniquely human.
	
	The impact of LLMs is profoundly dualistic, presenting both unprecedented opportunities and significant challenges. In fields ranging from software development to biomedical research\footcite{alnajjar2007}, these models are accelerating innovation by automating code generation, assisting in drug discovery, and offering powerful tools for data analysis. However, their deployment is not without peril. Critical issues such as inherent biases learned from training data, the potential for mass dissemination of misinformation,\footcite{alip2022} and the substantial environmental footprint of training these models demand urgent attention. Furthermore, the "black box" nature of many LLMs—where even their creators cannot fully explain the reasoning behind a specific output—poses a significant hurdle for transparency and accountability in critical applications.
	
	Looking ahead, the trajectory of LLM development points toward greater integration and capability, moving closer to what some envision as Artificial General Intelligence (AGI). Future iterations are expected to feature enhanced multimodality,\footcite{akbar2024} allowing them to seamlessly process and generate information across text, images, and audio. Concurrently, research is focused on improving their reasoning abilities, reducing computational costs, and increasing their factual accuracy through novel training techniques.\footcite{alhusain2016} The ultimate goal remains the responsible stewardship of this technology. Achieving a future where LLMs augment human potential safely and equitably will require a concerted effort from researchers, policymakers, and society at large to establish robust ethical frameworks and governance.
	\section{Sources of Data}
	\lipsum[3]
	\section{Methods of Data Collection}
	\lipsum[3]
	\section{Data Analysis Method}	
	\lipsum[3]
	or if summarized as in table~\ref{tab:contoh-t} below.
	
	\begin{table}[h!]
		\centering
		\caption{contoh tabel.}
		\label{tab:contoh-t}
		\begin{tabular}{llcc}
			\toprule
			\textbf{Framework} & \textbf{Bahasa} & \textbf{Pola Arsitektur} & \textbf{Tahun Rilis} \\
			\midrule
			Laravel & PHP & MVC & 2011 \\
			Django & Python & MVT (Mirip MVC) & 2005 \\
			\bottomrule
		\end{tabular}
	\end{table}